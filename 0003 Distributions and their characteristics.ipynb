{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean, Median and Mode with Garmin Vivofit Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "sns.set_palette(['#00A99D', '#F5CA0C', '#B6129F', '#76620C', '#095C57'])\n",
    "# turn of data table rendering\n",
    "pd.set_option('display.notebook_repr_html', False)\n",
    "plt.style.use('ggplot')\n",
    "pd.__version__\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 248 days of step data and vivofit goals\n",
    "data = pd.read_csv('data\\garmin-vivofit.csv', index_col='date')\n",
    "data.drop(data.columns[[2, 3]],axis=1,inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the steps and goal data\n",
    "data.steps.plot(kind='area', figsize=(8,5), color='#00A99D', alpha=.5)\n",
    "data.goal.plot()\n",
    "plt.ylabel('steps per day')\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the mean\n",
    "The arithmetic mean is the most commonly used measure of central tendency. The Greek letter $\\mu$ (mu) is used to represent the population mean. To calculate the mean, we sum up all values $x_0+x_1+x_n$ and divide it by the number of values $n$.\n",
    "\n",
    "$$\\mu= \\frac{1}{n}\\sum_{i=0}^n x_i$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean\n",
    "x, n = 0.0, 0\n",
    "\n",
    "for number_of_steps in data.steps:\n",
    "    x += number_of_steps\n",
    "\n",
    "n = len(data.steps)\n",
    "\n",
    "mean = x / n\n",
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also let pandas use NumPy's mean function to do the job\n",
    "data.steps.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or we can call NumPy's mean function ourselves\n",
    "np.mean(data.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas to get the mean for all columns at ones\n",
    "data.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the mean, togehter with the steps and goal data\n",
    "data.steps.plot(kind='area', color='#00A99D', alpha=.5, figsize=(8,5))\n",
    "data.goal.plot(legend=True)\n",
    "plt.plot([0, len(data.steps)],[mean, mean])\n",
    "plt.ylabel('steps per day')\n",
    "plt.text(25, mean-1200, r'$\\mu=' + str(int(math.floor(mean))) + '$', fontsize=14)\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the median\n",
    "The median is often a better measure of central tendency when we have extreme outliers. The median is the value in the middle after we sort the data. This is why outliers do not influence the median as much as they do the mean. If the number of observations $n$ is even, we have to take the mean of the two middle values. We calculate for a zero based index.\n",
    "\n",
    "\n",
    "$$n\\ is\\ odd:\\ \\ x_{median}=x_{\\frac{n-1}{2}}$$\n",
    "\n",
    "$$n\\ is\\ even:\\ \\ x_{median}=\\frac{x_\\frac{n-2}{2}+x_\\frac{n}{2}}{2}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median, n = 0.0, 0\n",
    "\n",
    "# Get the number of observations\n",
    "n = len(data.steps)\n",
    "    \n",
    "# order the data\n",
    "ordered_data = data.steps.sort_values()\n",
    "\n",
    "if n % 2 == 0:\n",
    "    # n is even\n",
    "    m1 = ordered_data.iloc[int((n - 2) / 2)]\n",
    "    m2 = ordered_data.iloc[int((n / 2))]\n",
    "    median = (m1 + m2) / 2.0\n",
    "else:\n",
    "    # n is odd\n",
    "    median = ordered_data.ix[(n - 2) / 2.0]\n",
    "\n",
    "median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, we can let pandas use NumPy's median function to do the job\n",
    "data.steps.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or we can call NumPy's median function ourselves\n",
    "np.median(data.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas to get the median for all columns at ones\n",
    "data.median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the Mode\n",
    "The mode is one or more values which occur most often in the series. This measure of central tendency is especially meaningful when you have a lot of repeated data points (like five-star ratings, day numbers). For the Garmin Vivofit data, however, this holds a problem, because we seldom have the same step count on a particular day. In that case we could categorize our data into range buckets, like 0-1000, 1000-2000 steps, and count the frequencies of the observations in the buckets. Choosing the right bucket size can be tricky, because we can obscure the data or miss the 'real' mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a lambda that assigns a bucket of size 1000 steps\n",
    "# to each of the step count in the data set\n",
    "bucket_size = 1000\n",
    "bucket_calculator = lambda x: int(x) / bucket_size * bucket_size\n",
    "\n",
    "data['bucket'] = data.steps.apply(bucket_calculator)\n",
    "bucket_min = data.bucket.min()\n",
    "bucket_max = data.bucket.max()\n",
    "bins = int((bucket_max-bucket_min) / bucket_size)\n",
    "\n",
    "data.bucket.hist(color='#00A99D', alpha=.5, bins=bins, figsize=(8,5))\n",
    "\n",
    "print('Mode:', data.groupby('bucket').steps.count().idxmax(), \\\n",
    "      'with bucket size', bucket_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Variance and Standard Deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "The data is a set of ten salaries, as used in the Udacity course 'Intro to Descriptive Statistics' lesson 4 on measures of variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'salaries': \n",
    "                     [33219, 36254, 38801, 46335, 46840, \n",
    "                      47596, 55130, 56863, 78070, 88830]})\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot(kind='bar', color='#00A99D', alpha=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the Variance\n",
    "The variance of a data set describes the average of the squared differences from the mean. In other words, it is a measure of how far each value in the data set is from the mean. The symbol for the variance of a population is $\\sigma^2$ (sigma squared) and for a sample we use $s^2$. We calculate the variance by summing the squared difference from the mean for each value. For the population, we divide by the number of values $n$ in the data set.\n",
    "\n",
    "$$population\\ variance:\\ \\sigma^2=\\frac{1}{n}\\sum_{i=0}^n(x_i-\\mu)^2$$\n",
    "\n",
    "For the sample we divide the summed up values by the degrees of freedom $n-1$ (also called Bessel's correction). We use $\\bar{x}$ (x bar) to symbolize our sample mean.\n",
    "\n",
    "$$sample\\ variance:\\ s^2=\\frac{1}{n-1}\\sum_{i=0}^n(x_i-\\bar{x})^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To calculate the population variance\n",
    "n = len(data.salaries)\n",
    "\n",
    "# first calculate the mean\n",
    "mean = data.salaries.mean()\n",
    "\n",
    "# Sum up the squared differences from the mean\n",
    "squared_deviations = 0\n",
    "for v in data.salaries:\n",
    "    squared_deviations += (v - mean) ** 2\n",
    "\n",
    "population_variance = squared_deviations / n\n",
    "population_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To calculate the variance if we only have a sample\n",
    "# First calculate the degrees of freedom (apply Bessel's correction)\n",
    "dof = n - 1\n",
    "sample_variance = squared_deviations / dof\n",
    "sample_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Of course we can use pandas to let NumPy do the job for us\n",
    "# The ddof parameter stands for Delta Degrees of Freedom\n",
    "population_variance = data.salaries.var(ddof=0)\n",
    "sample_variance = data.salaries.var() # ddof=1 by default in pandas\n",
    "\n",
    "population_variance, sample_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or call the NumPy var function ourselves\n",
    "population_variance = np.var(data.salaries) # ddof=0 by default in NumPy\n",
    "sample_variance = np.var(data.salaries, ddof=1)\n",
    "\n",
    "population_variance, sample_variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the Standard Deviation\n",
    "The standard deviation is a widely used normalized measure of spread of a data set. Unlike the variance, the standard deviation is using the same scale as our values; dollars in this case. In a normal distribution, about 95% of the values are within two standard deviations of the mean. We use the Greek letter sigma $\\sigma$ to symbolize the population standard deviation. \n",
    "\n",
    "$$population\\ standard\\ deviation:\\ \\sigma=\\sqrt{\\frac{1}{n}\\sum_{i=0}^n(x_i-\\mu)^2}\\ \\ =\\ \\ \\sqrt{\\sigma^2}$$\n",
    "\n",
    "We use the lowercase letter $s$ if we indicate the sample standard deviation.\n",
    "\n",
    "$$sample\\ standard\\ deviation:\\ s=\\sqrt{\\frac{1}{n-1}\\sum_{i=0}^n(x_i-\\bar{x})^2}\\ \\ =\\ \\ \\sqrt{s^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To calculate the population standard deviation\n",
    "# we first need to calculate the population variance again\n",
    "n = len(data.salaries)\n",
    "\n",
    "# first calculate the mean\n",
    "mean = data.salaries.mean()\n",
    "\n",
    "# Sum up the squared differences from the mean\n",
    "squared_deviations = 0\n",
    "for v in data.salaries:\n",
    "    squared_deviations += (v - mean) ** 2\n",
    "\n",
    "population_variance = squared_deviations / n\n",
    "\n",
    "# Square the variance\n",
    "population_standard_deviation = math.sqrt(population_variance)\n",
    "population_standard_deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To calculate the sample standard deviation\n",
    "# First calculate the degrees of freedom (apply Bessel's correction)\n",
    "dof = n - 1\n",
    "sample_variance = squared_deviations / dof\n",
    "\n",
    "# Square the variance\n",
    "sample_standard_deviation = math.sqrt(sample_variance)\n",
    "sample_standard_deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's use pandas to let NumPy do the job for us\n",
    "population_standard_deviation = data.salaries.std(ddof=0)\n",
    "sample_standard_deviation = data.salaries.std()\n",
    "\n",
    "population_standard_deviation, sample_standard_deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or call the NumPy std function ourselves\n",
    "population_standard_deviation = np.std(data.salaries)\n",
    "sample_standard_deviation = np.std(data.salaries, ddof=1)\n",
    "\n",
    "population_standard_deviation, sample_standard_deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Standard Normal Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We use a fictional data set of 10000 averge number of Facebook friends.\n",
    "facebook_mu = 190.0\n",
    "facebook_sigma = 36.0\n",
    "facebook_friends = np.random.normal(facebook_mu, facebook_sigma, 10000)\n",
    "\n",
    "# show first 12 samples\n",
    "facebook_friends[:12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Probability Density Function\n",
    "The total area under the Probability Density Function (pdf) is always 1.0. Roughly 68% of the values is within 1 standard deviation from the mean. About 95% falls within two standard deviations. We can determine the probability of finding a given value in the distribution by using the pdf. \n",
    "\n",
    "Let's say someone's got 227 Facebook friends. What is the probability of having this or less number of Facebook friends?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First take a look at the pdf and especially the green area under\n",
    "# the curve containing the probability of 227 Facebook friends or less.\n",
    "x = 227.0\n",
    "sns.distplot(facebook_friends, label='Facebook friends', kde=False, \n",
    "             fit=stats.norm, color='w')\n",
    "plt.text(x+5, .0003, '$x$='+str(x))\n",
    "\n",
    "x_plot = np.linspace(min(facebook_friends), x, 1000)\n",
    "y_plot = stats.norm.pdf(x_plot, facebook_mu, facebook_sigma)\n",
    "plt.fill_between(x_plot,  y_plot)\n",
    "c=plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To calculate the probability, we need the z score.\n",
    "zscore = (x - facebook_mu) / facebook_sigma\n",
    "zscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the probability by calling stats.norm.cdf\n",
    "# This is a computational z table lookup\n",
    "p = stats.norm.cdf(zscore)\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this means the probability of people having 227 Facebook friends or less is about 85%. Since the area under the curve adds up to 1, we can say that the probability of people having a value more than 227 Facebook friends is $1-p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability of having a value more than 227\n",
    "1 - p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From probability back to the actual value\n",
    "Let's assume we have a 21% chance of having a certain number of Facebook friends or more. What is the minimum number of Facebook friends we have in this case?\n",
    "\n",
    "We use the ppf function (inverse cdf or $F^{-1}$) - from probability to z score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 1 - .21\n",
    "z = stats.norm.ppf(p)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From z score to number of Facebook friends\n",
    "x = z * facebook_sigma + facebook_mu\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The green area under the curve containing the probability \n",
    "# of (roughly) 206 Facebook friends or more.\n",
    "sns.distplot(facebook_friends, label='Facebook friends', kde=False, \n",
    "             fit=stats.norm, color='w')\n",
    "plt.text(x+5, .0003, '$x$='+str(int(x)))\n",
    "\n",
    "x_plot = np.linspace(x, max(facebook_friends), 1000)\n",
    "y_plot = stats.norm.pdf(x_plot, facebook_mu, facebook_sigma)\n",
    "plt.fill_between(x_plot,  y_plot)\n",
    "c=plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate probability in between two values\n",
    "What is the probability of people having between 120 and 170 Facebook friends?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to know the proportion of the green area under the curve.\n",
    "x1 = 120.0\n",
    "x2 = 170.0\n",
    "sns.distplot(facebook_friends, label='Facebook friends', kde=False, \n",
    "             fit=stats.norm, color='w')\n",
    "plt.text(x1+5, .0003, '$x_1$='+str(x1))\n",
    "plt.text(x2+5, .0003, '$x_2$='+str(x2))\n",
    "\n",
    "x_plot = np.linspace(x1, x2, 1000)\n",
    "y_plot = stats.norm.pdf(x_plot, facebook_mu, facebook_sigma)\n",
    "plt.fill_between(x_plot,  y_plot)\n",
    "c=plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need the z score of x1\n",
    "z1 = (x1 - facebook_mu) / facebook_sigma\n",
    "z1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we calculate the probability for value x1 or less\n",
    "p1 = stats.norm.cdf(z1)\n",
    "p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we calculate the z score for x2\n",
    "z2 = (x2 - facebook_mu) / facebook_sigma\n",
    "z2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and agian the probabilty for value x2 or less\n",
    "p2 = stats.norm.cdf(z2)\n",
    "p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So the probability of having between x1 and x2 Facebook friends is\n",
    "# the probability having x2 minus the probability having x1\n",
    "p2 - p1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember an example with S&P 500 stock index. Let return is distributed as a normal random variable with $\\mu = 0.001$ and $\\sigma = 0.01$. In the following lines you can find the real data of this index.\n",
    "Return values are also calculated. \n",
    "\n",
    "You should find out \n",
    " * What is the real percent of losses greater than 5% per day?\n",
    " * What is the sample mean and variance? Compare it to the given in Example.\n",
    " * What is the median return?\n",
    " * What is a VaR for more risky strategy with 90% confidence (we expect the dayli loss < 10%) and investment of \\$100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and plot Close values of index\n",
    "data = pd.read_csv('data/GSPC.csv',sep=',')\n",
    "data.Close.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calculate the index returns like in the example\n",
    "fund_return = pd.Series(data.Close[1:].values - data.Close[:-1].values)/1000\n",
    "fund_return.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fund_return.hist(bins=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
